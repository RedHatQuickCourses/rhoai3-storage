= The Data "Glue" of RHOAI: From Credential Sprawl to Governed Connectivity
:navtitle: Introduction & Value
:toc: macro

// Antora metadata
:page-role: product-concept
:description: Understanding the business value of Data Connections and storage architecture in the AI Factory.

[.lead]
*Stop hardcoding keys. Stop moving data that does not need to move. Start building the glue.*

In the world of traditional software, we learned long ago that credentials do not belong in source code. We use secrets managers, IAM roles, and environment variables. We decouple configuration from code.

Yet in many AI and ML teams, sensitive keys still live inside notebooks. Massive datasets are copied from bucket to bucket "just to be safe." Storage is siloed so that data scientists and MLOps engineers cannot share a single "golden" dataset without duplication and drift.

This is **credential sprawl** and **data gravity**. And it is the primary blocker to scalable, compliant AI operations.

[NOTE]
.The Core Sales Objection
====
*"Why do we need RHOAI Data Connections when we can just use public APIs or hardcode our S3 keys in the notebook?"*

Because you cannot govern what you do not abstract. Public APIs put keys in developers' hands; RHOAI abstracts credentials into Kubernetes Secrets via **Data Connections**, ensuring compliance and secure rotation without breaking code. The same pipeline runs in Dev, Test, and Prod by swapping the connection reference—no environment-specific hardcoding.
====

== The Solution: Data Connectivity and Storage Architecture

The **Red Hat OpenShift AI (RHOAI)** data connectivity and storage architecture is the "glue" that ties your AI factory together. It provides two pillars:

* **Data Connections:** The gateway to external data (S3, URI, OCI). Credentials live in Kubernetes Secrets; pods receive them at runtime. Admins create templates; users consume them.
* **Cluster Storage:** The engine for internal persistence. **PVCs (Persistent Volume Claims)** give workbenches and pipelines a place to store data without constantly re-uploading to object storage—reducing latency and egress cost.

Together, they decouple *data location* from *compute location*. The same code runs everywhere; only the connection reference changes.

== Four Pillars of Value

By implementing RHOAI data connectivity and storage correctly, you unlock four capabilities that ad-hoc key management and copy-paste storage cannot provide:

=== 1. Security & Governance ("The Vault")

When developers manage their own keys, compliance becomes a guessing game. When keys are in notebooks or config files, rotation breaks production.

* **The win:** Data Connections abstract credentials into Kubernetes Secrets. Only the platform injects them into pods at runtime.
* **The benefit:** You achieve compliance by default. Rotate credentials without touching application code. Audit who has access to which connection.

=== 2. Hybrid Portability ("One Pipeline, Many Environments")

Copy-pasting bucket names and keys across Dev, Test, and Prod leads to drift and mistakes.

* **The win:** The same pipeline code runs everywhere. You swap the **Data Connection** reference (e.g., Dev S3 vs. Prod S3); the code does not change.
* **The benefit:** No environment-specific hardcoding. Promote pipelines with confidence.

=== 3. Performance & ROI ("Less Moving, More Computing")

Constantly re-uploading intermediate data to S3 burns time and egress. Cold-starting models from raw downloads is slow.

* **The win:** **Pipeline Workspaces** (PVCs) pass data between pipeline steps instantly, avoiding repeated uploads. **Modelcars** (OCI container images) cache models and reduce startup times.
* **The benefit:** Faster pipelines, lower egress cost, better GPU utilization.

=== 4. Real-Time Collaboration ("One Golden Dataset")

When each researcher has a private copy of a 500GB dataset, storage and sync become a nightmare.

* **The win:** RHOAI supports **ReadWriteMany (RWX)** storage classes. Multiple workbenches can mount the same "golden dataset" simultaneously—no duplication.
* **The benefit:** Data scientists and MLOps engineers work from the same source of truth.

== Your Mission: Master the Glue

In this course, you will not just read about connections and PVCs; you will **design and apply** a storage strategy. You will take on the role of a platform engineer tasked with breaking the cycle of credential sprawl and inefficient data transfer.

**You will execute the following technical workflow:**

 1.  **Architecture:** Understand how Data Connections inject credentials into pods, how pipeline artifacts flow between S3 and PVC workspaces, and the critical distinction between RWO and RWX storage.
 2.  **The Well-Lit Paths:** Choose the right pattern—Explorer (individual workbench), Collaborator (shared dataset), or Engineer (production serving with Modelcars).
 3.  **Taxonomy:** Master the glossary—Data Connection, PVC, Modelcar, Pipeline Workspace—and the RHOAI 3 shift (e.g., PostgreSQL and vector stores for Llama Stack).
 4.  **Lab Setup:** Configure storage classes, create a Data Connection using the RHOAI 3 protocol annotation, and deploy a model with the connection attached.
 5.  **Operations:** Troubleshoot connection failures, storage class issues, and access modes.

[IMPORTANT]
.Prerequisites
====
To successfully complete the hands-on sections of this course, you need:

* Access to a **Red Hat OpenShift AI 3** cluster (3.0+; 3.2 recommended).
* Permissions to create Secrets, PVCs, and to configure StorageClasses (or `cluster-admin`).
* The `oc` CLI tool installed in your terminal.
* Approximately **1 TB** of cluster storage recommended for labs; GPU operators installed if you will deploy model serving.
====

---
*Ready to design your storage strategy? Let's start with the architecture.*
