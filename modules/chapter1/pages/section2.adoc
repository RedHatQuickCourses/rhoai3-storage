= Lab: Automating the Data Supply Chain
:navtitle: Fast Track Lab
:imagesdir: ../images

*Stop clicking. Start engineering.*

To build a Data Supply Chain, you first need a "Warehouse" (Object Storage) and a "Brain" (Metadata Storage). In a real corporate environment, these already exist. In this lab, we will simulate that environment by deploying a real instance of **MinIO** and **MySQL** to your cluster in seconds.

Once the infrastructure is live, you will act as the **Platform Architect**. You will define the "Universal Adapters" (Data Connections) that allow your Data Scientists to consume these resources securely, without ever knowing the root passwords.

== Prerequisites
* Access to an OpenShift cluster with **RHOAI 3** installed.
* `cluster-admin` privileges.
* The `oc` CLI tool installed.


== Phase 1: The Infrastructure (Deploy the Factory)

We have packaged the "Plumbing" into a Fast Track script. This script will:
1.  Create the `rhoai-model-registry-lab` namespace.
2.  Deploy **MinIO** (S3-compatible storage).
3.  Deploy **MySQL** (Relational Database).
4.  Expose them internally to the cluster.

=== 1. Acquire the Infrastructure Code
Create a file named `setup.sh` and paste the following content. This simulates checking out the "Ops Team" repository.

[source,bash]
----
#!/bin/bash
# setup.sh: Automates the "Plumbing" phase.
set -e
NAMESPACE="rhoai-model-registry-lab"

echo "ðŸš€ Starting AI Supply Chain Infrastructure Setup..."

# 1. Create Namespace
if oc get project "$NAMESPACE" > /dev/null 2>&1; then
    echo "âœ” Namespace $NAMESPACE exists."
else
    oc new-project "$NAMESPACE"
fi

# 2. Deploy MinIO (The Vault)
echo "âž¤ Deploying MinIO..."
# (In a real lab, these would be git checkout files. Here we apply directly.)
oc apply -f https://raw.githubusercontent.com/RedHatQuickCourses/rhoai3-storage/main/manifests/minio-backend.yaml -n $NAMESPACE

# 3. Deploy MySQL (The Brain)
echo "âž¤ Deploying MySQL..."
oc apply -f https://raw.githubusercontent.com/RedHatQuickCourses/rhoai3-storage/main/manifests/mysql-backend.yaml -n $NAMESPACE

echo "âœ… Infrastructure Setup Complete!"
echo "MinIO URL: http://minio-service.$NAMESPACE.svc.cluster.local:9000"
echo "MySQL Host: mysql.$NAMESPACE.svc.cluster.local"
----
*(Note: For this lab, assume the YAML files referenced in the script are available locally or apply the provided sample files.)*

=== 2. Execute the Provisioning
Run the script to stand up the factory floor.

[source,bash]
----
chmod +x setup.sh
./setup.sh
----

**Validation:**
Check that your "Warehouse" is running.
[source,bash]
----
oc get pods -n rhoai-model-registry-lab
----
*Expected Output:* You should see `minio` and `mysql` pods in `Running` state.

== Phase 2: The Universal Adapter (GitOps Connections)

Now that the servers are running, we need to create the "Plugs" that allow RHOAI to connect to them. We will use a **GitOps approach** to ensure these connections are standardized and reproducible.

=== Step 1: Define the S3 Standard (Connection Type)
First, we define the "Socket" for our Corporate Data Lake. This tells the RHOAI Dashboard what fields to expect.

**File:** `01-s3-type.yaml`
[source,yaml]
----
apiVersion: dashboard.opendatahub.io/v1alpha
kind: ConnectionType
metadata:
  name: corporate-minio-standard
  labels:
    opendatahub.io/dashboard: "true"
spec:
  displayName: "Corporate MinIO Standard"
  category: "Object Storage"
  description: "Standard connection to the internal MinIO Data Lake."
  fields:
    - name: Endpoint
      type: text
      required: true
      envVar: AWS_S3_ENDPOINT
    - name: Bucket
      type: text
      required: true
      envVar: AWS_S3_BUCKET
    - name: Access Key
      type: text
      required: true
      envVar: AWS_ACCESS_KEY_ID
    - name: Secret Key
      type: password
      required: true
      envVar: AWS_SECRET_ACCESS_KEY
----
[source,bash]
----
oc apply -f 01-s3-type.yaml
----

=== Step 2: Create the Data Connection (The Instance)
Now, we create the actual connection for the "Finance Team" that points to the MinIO instance we just deployed. Notice we use the internal ClusterIP service URL.

**File:** `02-finance-connection.yaml`
[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: finance-data-lake
  namespace: rhoai-model-registry-lab
  labels:
    opendatahub.io/dashboard: "true" # <1> Visible in UI
    opendatahub.io/managed: "true"
  annotations:
    opendatahub.io/connection-type: "corporate-minio-standard" # <2> Links to our Type
    openshift.io/display-name: "Finance Data Lake"
type: Opaque
stringData:
  Endpoint: "http://minio-service.rhoai-model-registry-lab.svc.cluster.local:9000"
  Bucket: "finance-records"
  Access Key: "minio"
  Secret Key: "minio123"
----
<1> **Required Label:** Without this, the secret is invisible to RHOAI.
<2> **Binding:** This links the secret to the schema we defined in Step 1.

[source,bash]
----
oc apply -f 02-finance-connection.yaml
----

== Phase 3: The Brain (Database Connection)

RHOAI 3 requires databases for Model Registries. Let's wire up the MySQL instance.

=== Step 1: Define the Database Standard
**File:** `03-db-type.yaml`
[source,yaml]
----
apiVersion: dashboard.opendatahub.io/v1alpha
kind: ConnectionType
metadata:
  name: corporate-mysql
  labels:
    opendatahub.io/dashboard: "true"
spec:
  displayName: "Corporate MySQL"
  category: "Database"
  fields:
    - name: Host
      type: text
      envVar: DB_HOST
    - name: Port
      type: text
      defaultValue: "3306"
      envVar: DB_PORT
    - name: Username
      type: text
      envVar: DB_USER
    - name: Password
      type: password
      envVar: DB_PASSWORD
----
[source,bash]
----
oc apply -f 03-db-type.yaml
----

=== Step 2: Create the DB Connection
**File:** `04-registry-db.yaml`
[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: registry-db-connection
  namespace: rhoai-model-registry-lab
  labels:
    opendatahub.io/dashboard: "true"
  annotations:
    opendatahub.io/connection-type: "corporate-mysql"
    openshift.io/display-name: "Model Registry Database"
type: Opaque
stringData:
  Host: "mysql.rhoai-model-registry-lab.svc.cluster.local"
  Port: "3306"
  Username: "admin"
  Password: "mysql-admin"
----
[source,bash]
----
oc apply -f 04-registry-db.yaml
----

== Phase 4: Verification (The Payoff)

You have successfully used GitOps to wire up a complete Data Supply Chain. Let's verify it in the interface.

1.  Open the **Red Hat OpenShift AI Dashboard**.
2.  Navigate to **Data Science Projects**.
3.  Select the project **rhoai-model-registry-lab** (created by the script).
4.  Click the **Data connections** tab.

**Success Criteria:**
* [ ] You see **"Finance Data Lake"** listed under *Object Storage*.
* [ ] You see **"Model Registry Database"** listed under *Database*.
* [ ] The connection details are hidden from view, ensuring security.


**You have now built a standardized, automated data layer that can be replicated across any environment.**