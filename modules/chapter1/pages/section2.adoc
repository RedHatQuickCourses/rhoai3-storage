= Build It: Configuring the Data Foundation
:role: Hands-On Lab
:audience: Platform Engineers
:description: Step-by-step guide to configuring Storage Classes and creating your first managed Data Connection.

[.lead]
*Theory is over. It is time to pour the concrete.*

In this lab, you will configure the two foundational pillars of the RHOAI platform: the **Storage Classes** for persistence and the **Connection Types** for secure access. By the end of this session, you will have a cluster ready to host high-performance AI workloads.

[IMPORTANT]
.Prerequisites
====
* Access to an OpenShift cluster with **RHOAI 3** installed.
* `cluster-admin` privileges.
* An available S3 Bucket (AWS or MinIO) with Access Key/Secret Key.
====

== Step 1: Configure Storage Classes
First, we must define how the cluster handles persistence. We need to ensure users can select the right storage for the right job.

1.  Navigate to the RHOAI Dashboard: **Settings** -> **Cluster settings** -> **Storage classes**.
2.  **Set the Default:** Ensure your fastest RWO block storage is enabled and selected as "Default".
3.  **Enable Shared Storage:** Locate your **RWX-capable** storage class (e.g., `ocs-storagecluster-cephfs`). Toggle it to **Enabled**.
    * *Why?* Without this, your users cannot create shared team workspaces or run distributed training jobs.

== Step 2: Create a Connection Type
RHOAI 3 allows you to define templates ("Connection Types") that standardize how users connect to data.

1.  Navigate to **Settings** -> **Environment setup** -> **Connection types**.
2.  Click **Create connection type**.
3.  **Define the Schema:**
    * **Name:** `Corporate S3 Standard`
    * **Category:** `Object Storage`
    * **Fields:** Add fields for `Bucket`, `Endpoint`, `Access Key`, and `Secret Key`.
4.  **Preview:** Check the form to ensure it looks user-friendly for your data scientists.
5.  **Save.**

== Step 3: Create the Data Connection
Now, act as the Platform Engineer creating a secure connection for a project.

1.  Navigate to **Projects** and select a project (or create `my-ai-project`).
2.  Click **Data connections** -> **Add data connection**.
3.  Select the **Connection Type** you created (`Corporate S3 Standard`).
4.  **Fill in the details:**
    * Enter your real AWS/MinIO credentials.
    * *Note:* The user will see this connection name, but they will *not* need to copy-paste these credentials later.

== Step 4: Verification (The "Payoff")
Let's verify that the abstraction works.

1.  Create a **Workbench** in the same project.
2.  In the **Data connections** section of the workbench creation form, select your new connection: `Use existing data connection`.
3.  Launch the Workbench and open a Terminal.
4.  Run `env | grep AWS`.
    * **Success:** You should see `AWS_ACCESS_KEY_ID` and `AWS_S3_ENDPOINT` populated automatically.

[NOTE]
.The Result
====
You have successfully decoupled the data from the user. The data scientist can now use `boto3` to access the bucket without ever knowing the password. You have stopped credential sprawl before it started.
====

*Next Step: Now that your data foundation is secure, proceed to the "Pipeline Automation" module.*